# -*- coding: utf-8 -*-
"""GNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HuAeuMo2ICshIowloBmRsl-K36mKYed5
"""

import torch
import torch.nn as nn
import networkx as nx
import random
from torch.utils.data import Dataset, DataLoader
from transformers import BertConfig, BertModel
from sklearn.preprocessing import LabelEncoder
import numpy as np
#import the dependencies

# Set seeds for reproducibility
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# 1. Load graph and prepare labels
G = nx.karate_club_graph()
labels = [G.nodes[node]['club'] for node in G.nodes()]  # ['Mr. Hi', 'Officer', ...]
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(labels)  # Convert to 0,1
num_classes = len(label_encoder.classes_)

# 2. One-hot encode labels
one_hot_labels = torch.eye(num_classes)[encoded_labels]

# 3. Generate random walks of one-hot labels
from sklearn.model_selection import train_test_split
def generate_label_walks(G, one_hot_labels, num_walks=5, walk_length=8):
    walks = []
    for _ in range(num_walks):
        for node in G.nodes():
            walk = [one_hot_labels[node]]  # Start with node's one-hot label
            current = node
            for _ in range(walk_length - 1):
                neighbors = list(G.neighbors(current))
                if not neighbors:
                    break
                current = random.choice(neighbors)
                walk.append(one_hot_labels[current])
            walks.append(torch.stack(walk))  # Tensor of shape [walk_length, num_classes]
    return walks


# Generate all walks first, then split into train/test
walks = generate_label_walks(G, one_hot_labels)
train_walks, test_walks = train_test_split(walks, test_size=0.3, random_state=seed)

print(f"Generated {len(walks)} walks total")
print(f"Training walks: {len(train_walks)}, Test walks: {len(test_walks)}")
print(f"Generated {len(walks)} walks (sample shape: {walks[0].shape})")

walks

# 4. Create sequence prediction dataset
class LabelWalkDataset(Dataset):
    def __init__(self, walks, seq_length=4):
        self.seq_length = seq_length
        self.examples = []
        for walk in walks:
            for i in range(len(walk) - seq_length):
                input_seq = walk[i:i+seq_length]  # Shape: [seq_length, num_classes]
                target = walk[i+seq_length].argmax()  # Convert one-hot to class index
                self.examples.append((input_seq, target))

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        return {
            'input_features': self.examples[idx][0].float(),  # One-hot sequence
            'labels': self.examples[idx][1]                  # Class index
        }

# Create separate datasets for train and test
train_dataset = LabelWalkDataset(train_walks)
test_dataset = LabelWalkDataset(test_walks)

train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# 5. BERT model adapted for one-hot inputs
class BertEncoder(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        # Project one-hot vectors to BERT's hidden size
        self.projection = nn.Linear(num_classes, 768)

        # Initialize BERT
        config = BertConfig(
            hidden_size=768,
            num_attention_heads=4,
            num_hidden_layers=2

        )
        self.bert = BertModel(config)

        # Classifier head
        self.classifier = nn.Linear(768, num_classes)

    def forward(self, input_features, labels=None):
        # input_features shape: [batch, seq_len, num_classes]
        projected = self.projection(input_features)  # [batch, seq_len, 768]

        # BERT processing
        outputs = self.bert(
            inputs_embeds=projected,
            attention_mask=torch.ones(projected.shape[:2], device=input_features.device))

        # Use [CLS] token for classification
        logits = self.classifier(outputs.last_hidden_state[:, 0, :])

        loss = None
        if labels is not None:
            loss = nn.CrossEntropyLoss()(logits, labels)
        return {'logits': logits, 'loss': loss}

# 6. Training setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertEncoder(num_classes).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# 7. Training loop
# Re-instantiate the BertEncoder model to ensure it's the correct one
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertEncoder(num_classes).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)


for epoch in range(10):
    model.train()
    total_loss = 0
    for batch in train_dataloader: # Use the train dataloader for the BertEncoder model
        inputs = batch['input_features'].to(device)
        targets = batch['labels'].to(device)

        outputs = model(inputs, targets) # Call the BertEncoder model's forward method
        loss = outputs['loss']

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}: Loss = {total_loss/len(train_dataloader):.4f}")

# 8. Evaluation Function
def evaluate_model(model, dataloader, label_encoder):
    model.eval()
    total_correct = 0
    total_samples = 0
    confusion_matrix = torch.zeros(len(label_encoder.classes_),
                                 len(label_encoder.classes_))

    with torch.no_grad():
        for batch in dataloader:
            inputs = batch['input_features'].to(device)
            targets = batch['labels'].to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs['logits'], 1)

            total_correct += (preds == targets).sum().item()
            total_samples += targets.size(0)

            for t, p in zip(targets.view(-1), preds.view(-1)):
                confusion_matrix[t.long(), p.long()] += 1

    accuracy = total_correct / total_samples
    print(f"\nEvaluation Results:")
    print(f"Accuracy: {accuracy:.4f}")
    print("Confusion Matrix:")
    print(confusion_matrix)

    # Calculate precision, recall, F1 for each class
    for i, class_name in enumerate(label_encoder.classes_):
        tp = confusion_matrix[i,i].item()
        fp = confusion_matrix[:,i].sum().item() - tp
        fn = confusion_matrix[i,:].sum().item() - tp

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        print(f"\nClass: {class_name}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")

    return accuracy

# 9. Run Evaluation
test_accuracy = evaluate_model(model, test_dataloader, label_encoder)

# 10. Visualization of Sample Predictions
import matplotlib.pyplot as plt

def visualize_predictions(model, dataset, label_encoder, num_samples=5):
    model.eval()
    samples = random.sample(range(len(dataset)), num_samples)

    plt.figure(figsize=(15, 5))
    for i, sample_idx in enumerate(samples):
        data = dataset[sample_idx]
        inputs = data['input_features'].unsqueeze(0).to(device)

        with torch.no_grad():
            outputs = model(inputs)
            probs = torch.softmax(outputs['logits'], dim=1)
            pred = probs.argmax().item()
            true_label = data['labels'].item()

        # Convert one-hot sequence back to labels
        input_seq = data['input_features'].argmax(dim=1)
        input_labels = label_encoder.inverse_transform(input_seq.cpu().numpy())

        plt.subplot(1, num_samples, i+1)
        plt.bar(label_encoder.classes_, probs.squeeze().cpu().numpy())
        plt.title(f"Input: {input_labels}\nTrue: {label_encoder.classes_[true_label]}\nPred: {label_encoder.classes_[pred]}")
        plt.ylim([0, 1])

    plt.tight_layout()
    plt.show()

# Visualize random predictions
visualize_predictions(model, test_dataset, label_encoder)

# Visualize random predictions from test set
visualize_predictions(model, test_dataset, label_encoder)

#finding graph features

print(f"Number of nodes: {G.number_of_nodes()}")
print(f"Number of edges: {G.number_of_edges()}")

# Degree Centrality: Fraction of nodes a node is connected to.
degree_centrality = nx.degree_centrality(G)
print("\nDegree Centrality (top 5 nodes):")

# Commented out IPython magic to ensure Python compatibility.
# %pip install igraph

# The 'degree' method directly returns the degrees.
degree_centrality = G.degree()
degree_centrality

# Load the Karate Club dataset
import igraph as ig
g = ig.Graph.Famous("Zachary")

g.vs["degree"] = g.degree()
g.vs["betweenness"] = g.betweenness()
g.vs["closeness"] = g.closeness()
g.vs["eigenvector"] = g.eigenvector_centrality()
g.vs["pagerank"] = g.pagerank()
communities = g.community_multilevel()
g.vs["community"] = communities.membership

for node in g.vs:
    print(f"Node {node.index}:")
    print(f"  Club: {node['club'] if 'club' in node.attributes() else 'N/A'}")

    # Safely print calculated features
    print(f"  Degree: {node['degree'] if 'degree' in node.attributes() else 'N/A'}")
    print(f"  Betweenness: {node['betweenness'] if 'betweenness' in node.attributes() else 'N/A'}")
    print(f"  Closeness: {node['closeness'] if 'closeness' in node.attributes() else 'N/A'}")
    print(f"  Eigenvector: {node['eigenvector'] if 'eigenvector' in node.attributes() else 'N/A'}")
    print(f"  PageRank: {node['pagerank'] if 'pagerank' in node.attributes() else 'N/A'}")
    print(f"  Community: {node['community'] if 'community' in node.attributes() else 'N/A'}\n")

"""Model #2 input is centrality measures of the previous nodes and o/p is labels"""

# Build feature matrix [N, F]
features = np.array([
    [v["degree"], v["betweenness"], v["closeness"],
     v["eigenvector"], v["pagerank"], v["community"]]
    for v in g.vs  # Use the igraph graph 'g' instead of networkx graph 'G'
], dtype=np.float32)

# Normalize features to [0,1]
from sklearn.preprocessing import MinMaxScaler
features = MinMaxScaler().fit_transform(features).astype(np.float32)
feat_dim = features.shape[1]

def generate_feature_label_walks(G, features, encoded_labels, num_walks=5, walk_length=8):

    pairs = []
    for _ in range(num_walks):
        for node in G.nodes():
            f_seq = [features[node]]
            l_seq = [encoded_labels[node]]
            cur = node
            for _ in range(walk_length - 1):
                nbrs = list(G.neighbors(cur))
                if not nbrs:
                    break
                cur = random.choice(nbrs)
                f_seq.append(features[cur])
                l_seq.append(int(encoded_labels[cur]))
            if len(f_seq) == walk_length:
                pairs.append((np.stack(f_seq, axis=0), np.array(l_seq, dtype=np.int64)))
    return pairs

# Hyperparams for data
NUM_WALKS   = 10
WALK_LENGTH = 8

pairs = generate_feature_label_walks(G, features, encoded_labels,
                                     num_walks=NUM_WALKS, walk_length=WALK_LENGTH)
len(pairs), pairs[0][0].shape, pairs[0][1].shape  # sanity

# 70/15/15 split
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.preprocessing import MinMaxScaler
# Split data into train/val/test (70%/15%/15%)
train_pairs, temp_pairs = train_test_split(pairs, test_size=0.30, random_state=42)
val_pairs, test_pairs = train_test_split(temp_pairs, test_size=0.50, random_state=42)

# Collect features only from train set
train_feats = np.vstack([x for x, _ in train_pairs])

# Fit scaler only on train
scaler = MinMaxScaler().fit(train_feats)


# Apply to all splits
def scale_pairs(pairs, scaler):
    return [(scaler.transform(x), y) for x, y in pairs]

train_pairs = scale_pairs(train_pairs, scaler)
val_pairs   = scale_pairs(val_pairs, scaler)
test_pairs  = scale_pairs(test_pairs, scaler)

# Calculate class weights to handle imbalance
all_train_labels = []
for pair in train_pairs:
    x, y = pair
    all_train_labels.extend(y)  # Collect all individual labels from sequences

class_counts = torch.bincount(torch.tensor(all_train_labels))
class_weights = 1.0 / class_counts.float()
class_weights = class_weights / class_weights.sum() * len(class_weights)

class CentralityToLabelDataset(Dataset):
    def __init__(self, pairs):
        self.pairs = pairs

    def __len__(self):
        return len(self.pairs)  # This tells DataLoader how many samples exist

    def __getitem__(self, idx):
        x, y = self.pairs[idx]
        return {"x": torch.tensor(x, dtype=torch.float32), "y": torch.tensor(y, dtype=torch.long)}

# Create data loaders for batch processing
train_loader = DataLoader(CentralityToLabelDataset(train_pairs), batch_size=16, shuffle=True)
val_loader = DataLoader(CentralityToLabelDataset(val_pairs), batch_size=16)
test_loader = DataLoader(CentralityToLabelDataset(test_pairs), batch_size=16)

# Seq2Seq model with BERT encoder
class Seq2SeqBERT(nn.Module):
    def __init__(self,
                 input_dim: int,
                 num_classes: int,
                 hidden_dim: int = 64,
                 num_layers: int = 1,
                 nhead: int = 2,
                 max_len = WALK_LENGTH,
                 dropout_rate: float = 0.1):

        super().__init__()
        self.num_classes = num_classes
        self.hidden_dim  = hidden_dim
        self.max_len     = max_len
        self.dropout = nn.Dropout(dropout_rate)

        # ----- Encoder (BERT) -----
        self.src_proj = nn.Linear(input_dim, hidden_dim)  # project features -> hidden
        enc_cfg = BertConfig(
            hidden_size=hidden_dim,
            num_attention_heads=nhead,
            num_hidden_layers=num_layers,
            intermediate_size=hidden_dim * 4,
            add_cross_attention=False,
            hidden_dropout_prob=dropout_rate,
            attention_probs_dropout_prob=dropout_rate
        )
        self.encoder = BertModel(enc_cfg)

        # ----- Decoder (Transformer) -----
        self.decoder_layer = nn.TransformerDecoderLayer(
            d_model=hidden_dim, nhead=nhead, dim_feedforward=hidden_dim * 4, batch_first=False, dropout=dropout_rate
        )
        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)

        # Label token embedding (+1 for BOS)
        self.BOS_IDX = num_classes
        self.tok_embed = nn.Embedding(num_embeddings=num_classes + 1, embedding_dim=hidden_dim)

        # Learnable positional embeddings for decoder
        self.pos_embed = nn.Embedding(num_embeddings=max_len, embedding_dim=hidden_dim)

        # Final classifier (hidden -> label logits)
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, num_classes)
        )

    def _causal_mask(self, T: int, device):
        # Prevent attending to future tokens at the decoder
        return nn.Transformer.generate_square_subsequent_mask(T).to(device)

    def forward(self, src_feats: torch.Tensor, tgt_in: torch.Tensor):
        """
        Training with teacher-forcing.
        src_feats: [B, S, F]  - feature sequence
        tgt_in:    [B, T]     - decoder input tokens (BOS + gold[:-1])
        Returns logits: [B, T, C]
        """
        B, S, F = src_feats.shape
        _, T    = tgt_in.shape

        # Apply dropout to input features
        src_feats = self.dropout(src_feats)

        # ----- Encode source -----
        src_proj = self.src_proj(src_feats)                     # [B, S, H]
        src_mask = torch.ones(B, S, device=src_feats.device)    # all valid tokens
        enc_out  = self.encoder(inputs_embeds=src_proj,
                                attention_mask=src_mask).last_hidden_state  # [B, S, H]
        enc_out  = enc_out.transpose(0, 1)                      # -> [S, B, H] for decoder

        # ----- Prepare decoder inputs (tokens + positions) -----
        tok_emb = self.tok_embed(tgt_in)                        # [B, T, H]
        pos_ids = torch.arange(T, device=src_feats.device).unsqueeze(0).expand(B, T)
        dec_in  = (tok_emb + self.pos_embed(pos_ids)).transpose(0, 1)  # [T, B, H]

        # Causal mask for autoregression
        tgt_mask = self._causal_mask(T, device=src_feats.device)

        # ----- Decode -----
        dec_out = self.decoder(dec_in, enc_out, tgt_mask=tgt_mask)  # [T, B, H]
        dec_out = self.dropout(dec_out)
        logits  = self.classifier(dec_out.transpose(0, 1))          # [B, T, C]
        return logits

    @torch.no_grad()
    def generate(self, src_feats: torch.Tensor, max_new_tokens: int, beam_width: int = 1):
        """
        Greedy decoding (beam_width=1) or Beam search.
        src_feats: [B, S, F]
        Returns predicted labels: [B, max_new_tokens]
        """
        B, S, F = src_feats.shape
        device  = src_feats.device

        # Encode source (repeat for beam search)
        src_proj = self.src_proj(src_feats)                     # [B, S, H]
        src_mask = torch.ones(B, S, device=device)
        enc_out  = self.encoder(inputs_embeds=src_proj,
                                attention_mask=src_mask).last_hidden_state
        enc_out  = enc_out.transpose(0, 1)                      # [S, B, H]

        # Initialize beams
        # Each beam is a tuple: (sequence_of_tokens, log_probability)
        beams = [[(torch.full((1, 1), self.BOS_IDX, dtype=torch.long, device=device), 0.0)] for _ in range(B)]

        # Autoregressively append tokens
        for _ in range(max_new_tokens):
            next_beams = [[] for _ in range(B)]
            for batch_idx in range(B):
                for seq, log_prob in beams[batch_idx]:
                    T = seq.size(1)
                    tok_emb = self.tok_embed(seq)                        # [1, T, H]
                    pos_ids = torch.arange(T, device=device).unsqueeze(0).expand(1, T)
                    dec_in  = (tok_emb + self.pos_embed(pos_ids)).transpose(0, 1)  # [T, 1, H]

                    # Select corresponding encoder output for this beam
                    enc_out_beam = enc_out[:, batch_idx, :].unsqueeze(1) # [S, 1, H]

                    tgt_mask = self._causal_mask(T, device=device)

                    dec_out = self.decoder(dec_in, enc_out_beam, tgt_mask=tgt_mask)  # [T, 1, H]
                    step_logits = self.classifier(dec_out[-1])                  # [1, C]
                    log_probs = torch.log_softmax(step_logits, dim=-1)          # [1, C]

                    # Expand beam
                    topk_log_probs, topk_indices = torch.topk(log_probs, beam_width, dim=-1)

                    for k in range(beam_width):
                        next_token = topk_indices[0, k].unsqueeze(0).unsqueeze(0) # [1, 1]
                        new_seq = torch.cat([seq, next_token], dim=1)             # [1, T+1]
                        new_log_prob = log_prob + topk_log_probs[0, k].item()
                        next_beams[batch_idx].append((new_seq, new_log_prob))

                # Select top beam_width sequences for this batch item
                next_beams[batch_idx].sort(key=lambda x: x[1], reverse=True)
                beams[batch_idx] = next_beams[batch_idx][:beam_width]

        # Return the best sequence from each beam (highest log probability)
        final_sequences = torch.cat([beam[0][0] for beam in beams], dim=0)
        return final_sequences[:, 1:]  # drop BOS

# Instantiate model, optimizer, loss
model = Seq2SeqBERT(
    input_dim=feat_dim,
    num_classes=num_classes,
    hidden_dim=128,
    num_layers=2,
    nhead=4,
    max_len=WALK_LENGTH
).to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))

def evaluate(model, dataloader, criterion, device, num_classes):
    model.eval()
    total_loss, total_tokens, total_tok_correct = 0.0, 0, 0
    total_seqs, total_seq_correct = 0, 0

    with torch.no_grad():
        for batch in dataloader:
            x = batch["x"].to(device)  # [B, S, F]
            y = batch["y"].to(device)  # [B, T]

            # Teacher forcing input: [BOS, y[:-1]]
            bos = torch.full((y.size(0), 1), fill_value=model.BOS_IDX,
                             dtype=torch.long, device=device)
            y_in = torch.cat([bos, y[:, :-1]], dim=1)  # [B, T]

            outputs = model(x, y_in)  # [B, T, C]
            loss = criterion(outputs.reshape(-1, num_classes), y.reshape(-1))
            total_loss += loss.item()

            # Token accuracy
            preds = outputs.argmax(dim=-1)  # [B, T]
            total_tok_correct += (preds == y).sum().item()
            total_tokens += y.numel()

            # Sequence accuracy (all tokens correct)
            total_seqs += y.size(0)
            total_seq_correct += ((preds == y).all(dim=1)).sum().item()


    avg_loss = total_loss / max(1, len(dataloader))
    tok_acc  = total_tok_correct / max(1, total_tokens)
    seq_acc  = total_seq_correct / max(1, total_seqs)
    return avg_loss, tok_acc, seq_acc


EPOCHS = 60
patience = 10  # Number of epochs to wait for improvement
best_val_loss = float('inf')
epochs_no_improve = 0

for epoch in range(1, EPOCHS + 1):
    model.train()
    total_loss = 0.0
    correct = 0  # Initialize correct predictions count
    total = 0    # Initialize total tokens count
    batch_count = 0 # Initialize batch count


    for batch in train_loader:
        x = batch["x"].to(device)  # [B, S, F]
        y = batch["y"].to(device)  # [B, T]

        # Forward pass
        optimizer.zero_grad()
        # outputs = model(x)  # [Batch, Time, Num_Classes] - Incorrect call
        # Correct call for Seq2SeqBERT with teacher forcing
        bos = torch.full((y.size(0), 1), fill_value=model.BOS_IDX, dtype=torch.long, device=device)
        y_in = torch.cat([bos, y[:, :-1]], dim=1) # [B, T]
        outputs = model(x, y_in) # [Batch, Time, Num_Classes]


         # Loss calculation (class weights applied automatically)
        loss = criterion(outputs.reshape(-1, outputs.size(-1)), y.reshape(-1))

        # Backward pass
        loss.backward()
        optimizer.step()

         # Metrics tracking (token accuracy)
        _, predicted = outputs.max(-1)
        total += y.numel()  # Total tokens in the batch
        correct += (predicted == y).sum().item()  # Correct predictions in the batch
        total_loss += loss.item()
        batch_count += 1


    train_loss = total_loss / max(1, batch_count) # Use batch_count for average loss
    train_tok_acc = 100 * correct / max(1, total) # Calculate token accuracy

    val_loss, val_tok_acc, val_seq_acc = evaluate(model, val_loader, criterion, device, num_classes)

    print(f"Epoch {epoch:02d} | TrainLoss {train_loss:.4f} | TrainTokAcc {train_tok_acc:.2f}%"
          f"| ValLoss {val_loss:.4f} | ValTokAcc {val_tok_acc:.4f} | ValSeqAcc {val_seq_acc:.4f}")

    # Early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        epochs_no_improve = 0
    else:
        epochs_no_improve += 1
        if epochs_no_improve == patience:
            print(f"Early stopping triggered after {patience} epochs with no improvement.")
            break

test_loss, test_tok_acc, test_seq_acc = evaluate(model, test_loader, criterion, device, num_classes)
print(f"\nTEST → Loss: {test_loss:.4f} | TokAcc: {test_tok_acc:.4f} | SeqAcc: {test_seq_acc:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

model.eval()

def analyze_predictions(model, dataloader, label_encoder, device, num_samples=5):
    """
    Comprehensive analysis of model predictions with visualization
    """
    model.eval()
    all_true = []
    all_preds = []
    sample_results = []

    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            x = batch["x"].to(device)
            y_true = batch["y"].to(device)

            preds = model.generate(x, max_new_tokens=y_true.size(1))


            # Store for overall metrics
            all_true.extend(y_true.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())

            # Store sample results for detailed analysis
            if batch_idx == 0 and len(sample_results) < num_samples:
                for i in range(min(num_samples, len(x))):
                    sample_results.append({
                        'features': x[i].cpu().numpy(),
                        'true_labels': y_true[i].cpu().numpy(),
                        'pred_labels': preds[i].cpu().numpy()
                    })

    # Convert to numpy arrays
    all_true = np.array(all_true)
    all_preds = np.array(all_preds)

    # Calculate metrics
    token_accuracy = np.mean(all_true == all_preds)
    sequence_accuracy = np.mean(np.all(all_true == all_preds, axis=1))

    print("=" * 60)
    print("MODEL PREDICTION ANALYSIS")
    print("=" * 60)
    print(f"Token-level accuracy: {token_accuracy:.4f}")
    print(f"Sequence-level accuracy: {sequence_accuracy:.4f}")
    print()

    # Detailed sample analysis
    print("DETAILED SAMPLE ANALYSIS:")
    print("-" * 40)

    for i, sample in enumerate(sample_results):
        true_labels = sample['true_labels']
        pred_labels = sample['pred_labels']

        print(f"Sample {i+1}:")
        print(f"True:  {true_labels}")
        print(f"Pred:  {pred_labels}")

        # Calculate per-position accuracy
        correct_positions = true_labels == pred_labels
        position_accuracy = np.mean(correct_positions)

        print(f"Match: {['✗', '✓'][int(np.all(correct_positions))]} "
              f"(Position accuracy: {position_accuracy:.2f})")

        # Show which positions were wrong
        wrong_positions = np.where(~correct_positions)[0]
        if len(wrong_positions) > 0:
            print(f"Wrong positions: {wrong_positions}")
            for pos in wrong_positions:
                print(f"  Position {pos}: True={true_labels[pos]}, Pred={pred_labels[pos]}")

        print("-" * 40)


    # Classification report
    print("\nCLASSIFICATION REPORT:")
    print(classification_report(all_true.flatten(), all_preds.flatten(),
                               target_names=label_encoder.classes_, zero_division=0))


    return all_true, all_preds

# Usage
all_true, all_preds = analyze_predictions(model, test_loader, label_encoder, device, num_samples=5)